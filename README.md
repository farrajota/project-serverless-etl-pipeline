# Case Study - Gender Service

This repo contains the code to deploy a serverless data pipeline using aws' Kinesis, S3, Dynamodb, Lambda and API Gateway services.

The architecture is coded as a CloudFormation template for easier deployment to the aws cloud.

## Pipeline overview

The pipeline uses only "serverless" services in AWS to store and process streams of data from visitor's page hits. This pipeline also provides a gender API service to fetch the gender of visitors via an identifier passed a `GET` request.

The architecture of the pipeline in this repo is displayed in the image below. This architecture requires little maitenance and is capable of handling large loads of data by elasticaly scaling from dozens to thousands of requests per second without special oversight.

<p align="center"><img src="img/serverless_pipeline.png" alt="Serverless pipeline architecture" height="50%" width="50%"></p>

## Requirements

To deploy the architecture of the pipeline, it is required the following tools in order to set it up:

- Linux
- AWS account
- Python 3.6+ (pip)
- [docker](https://www.docker.com/)
- [AWS CLI](https://github.com/aws/aws-cli)
- [AWS SAM](https://github.com/awslabs/serverless-application-model)

### Installing docker

To install docker, follow this guide in the official docker documentation: https://docs.docker.com/install/linux/docker-ce/ubuntu/

### Installing AWS CLI

Installing the aws cli is pretty straight forward. To do so, run the following command in a terminal:

```bash
pip install awscli
```

### Installing AWS SAM

Installing AWS Serverless Application Model (SAM) is rather simple and requires you to have docker, aws cli and pip installed in your machine. To install AWS SAM you can follow the official install guide in AWS: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html

### Getting an AWS account for programmatic access

If you do not have an aws account you first need to create one before proceeding any further. To do so, follow the official guide here: https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/

When you get access to AWS services, create an account for programmatic access (and console access as well) in order to be able to deploy the pipeline in the aws cloud. This short [youtube video](https://www.youtube.com/watch?v=TbzbrkaB_Uo) should help you create one in case you don't know yet how to do it.

## Getting started

Before proceeding to deploy the architecture, you will need to define two environment vars containing your aws's access key and secrets. You can set them up by doing the following:

```bash
export AWS_ACCESS_KEY=<your_access_key>
export AWS_SECRET_KEY=<your_secret_key>
```

After this is done, in order to deploy and test the architecture you need to build, package and deploy the architecture to aws cloud. Then, to really test the architecture, some dummy data is needed to be generated in order to see the pipeline working.

To simplify these processs, several macros are available in the `Makefile` in the root of this repo.

The next sections follows the 4 steps to start using and testing the pipeline:

1. Build the code
2. Package the code and generate a cloudformation template
3. Deploy the cloudformation template to aws cloud
4. Generate dummy data to test the pipeline

### Building the source code

The first step to deploy the pipeline starts with building the lambda code stored in the following folders:

- `kinesis_firehose_stream_process/`: contains the code used to preprocess the data stream generated by Kinesis Firehose
- `process_stream_data/`: contains the code used to compress the stream data to [parquet](https://parquet.apache.org/) format, process the visitor's gender hits and update the dynamodb table state
- `daily_process_lambda/`: computes the daily page hits of all visitors on the previous day, computes the top gender of the last 7 days of visitors and updates the dynamodb table state
- `gender_api_lambda/`: retrieves the gender data of a visitor via a `clientid` from the dynamodb table

To to this, run the following command in the terminal:

```bash
make build
```

### Packaging the code

Next, the code and its dependencies need to be bundled and stored in a bucket in order for aws lambda to access it, and to transform the `template.yaml` to a valid cloudformation template.

To package and generate the cloudformation template, run the following command in the terminal:

```bash
make package
```

### Deploying the code

Finally, the architecture can be deployed to the aws cloud by using the cloudformation template generated in the previous step:

```bash
make deploy
```

This will take a few minutes for aws to to spin the services. You can check its progress by visiting the CloudFormation service in the aws console. When the services and ready to run, the command shall stop and you should be able to run additional commands in the terminal.

### All-in-one command

You can accomplish the previous commands with the following macro to build, package and deploy the pipeline with a single command:

```bash
make setup-all
```

### Generating dummy data

To see the pipeline working in its full potential, it would be best to generate data of some previous days in order to see the effects of computing the top gender in the last 7 days. To do so, run the following command in the terminal which generate some dummy data for the last 10 days (from the current time ofc):

```bash
make generate-dummy-data
```

### Generating streams of data

To generate streams of dummy data you can run the following command

```bash
make generate-stream-data
```

You can define the number of requests per second by using the `requests_per_second` input arg:

```bash
make generate-stream-data requests_per_second=50
```

### Cleaning up

After you tested and experimented with the pipeline, you can stop and delete the cloudformation stack with the following command:

```bash
make delete-stack
```

> Note: S3 buckets containing files won't be deleted by default, so you might have to manually delete them to clean your environment.

### Unit tests

You can run the unit tests of the lambda functions used to process the stream data by running the following command in the terminal:

```bash
make test
```

This will run all unit tests inside the `tests/` folder.

> Note: `pytest` module is required to run the tests.

# License

[MIT](LICENSE)